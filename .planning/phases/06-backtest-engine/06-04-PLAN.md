---
phase: 06-backtest-engine
plan: 04
type: execute
wave: 3
depends_on: ["06-02"]
files_modified:
  - src/bot/dashboard/routes/pages.py
  - src/bot/dashboard/routes/api.py
  - src/bot/dashboard/templates/backtest.html
  - src/bot/dashboard/templates/partials/backtest_form.html
  - src/bot/dashboard/templates/partials/equity_curve.html
  - src/bot/dashboard/templates/partials/param_heatmap.html
  - src/bot/dashboard/templates/base.html
  - src/bot/dashboard/app.py
autonomous: true

must_haves:
  truths:
    - "Dashboard has a /backtest page accessible from the navigation bar"
    - "User can configure and run a backtest from the dashboard (symbol, dates, strategy mode, parameters)"
    - "Dashboard shows backtest results with an interactive equity curve chart (BKTS-04)"
    - "Dashboard shows parameter comparison heatmap for sweep results (BKTS-04)"
    - "Backtest runs as a background task without blocking the dashboard"
    - "Results display net P&L, Sharpe ratio, max drawdown, win rate, and comparison table"
  artifacts:
    - path: "src/bot/dashboard/templates/backtest.html"
      provides: "Full backtest page with form, results area, and chart containers"
      contains: "equity-chart"
    - path: "src/bot/dashboard/templates/partials/equity_curve.html"
      provides: "Chart.js equity curve partial"
      contains: "Chart"
    - path: "src/bot/dashboard/templates/partials/param_heatmap.html"
      provides: "Parameter heatmap table partial"
      contains: "heatmap"
    - path: "src/bot/dashboard/routes/api.py"
      provides: "Backtest API endpoints (run, status, results)"
      contains: "backtest"
  key_links:
    - from: "src/bot/dashboard/routes/api.py"
      to: "src/bot/backtest/runner.py"
      via: "run_backtest() called as asyncio background task"
      pattern: "run_backtest"
    - from: "src/bot/dashboard/templates/backtest.html"
      to: "src/bot/dashboard/routes/api.py"
      via: "HTMX form submission to /api/backtest/run"
      pattern: "hx-post.*backtest"
    - from: "src/bot/dashboard/templates/partials/equity_curve.html"
      to: "Chart.js CDN"
      via: "JavaScript chart rendering"
      pattern: "new Chart"
---

<objective>
Build the dashboard backtest page with configuration form, equity curve chart, parameter heatmap, and background task execution.

Purpose: Satisfies BKTS-04 (dashboard displays backtest results with equity curve and parameter heatmap). Provides a visual interface for running backtests, viewing results, and comparing strategies without using the CLI.

Output: New /backtest page with HTMX form, Chart.js equity curve, HTML table heatmap, and API endpoints for background backtest execution.
</objective>

<execution_context>
@/Users/luckleineschaars/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luckleineschaars/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-backtest-engine/06-02-SUMMARY.md

@src/bot/dashboard/app.py
@src/bot/dashboard/routes/pages.py
@src/bot/dashboard/routes/api.py
@src/bot/dashboard/templates/base.html
@src/bot/dashboard/templates/index.html
@src/bot/dashboard/templates/partials/config_form.html
@src/bot/backtest/models.py
@src/bot/backtest/runner.py
@src/bot/backtest/sweep.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add backtest API endpoints and page route</name>
  <files>
    src/bot/dashboard/routes/api.py
    src/bot/dashboard/routes/pages.py
    src/bot/dashboard/app.py
  </files>
  <action>
**src/bot/dashboard/routes/api.py**: Add backtest API endpoints.

Add these endpoints to the existing API router:

1. `POST /api/backtest/run`:
   - Parse JSON body with fields: `symbol` (str), `start_date` (str YYYY-MM-DD), `end_date` (str YYYY-MM-DD), `strategy_mode` (str), and optional parameter overrides
   - Convert date strings to millisecond timestamps
   - Create BacktestConfig from the body
   - Start backtest as background task: `task = asyncio.create_task(run_backtest(config, db_path))`. Store the task and a unique task_id on `request.app.state.backtest_tasks` (a dict).
   - Return `JSONResponse({"task_id": task_id, "status": "running"})` immediately
   - Import `from bot.backtest.runner import run_backtest` and `from bot.backtest.models import BacktestConfig`

2. `POST /api/backtest/sweep`:
   - Parse JSON body with: `symbol`, `start_date`, `end_date`, `strategy_mode`, optional `param_grid` (dict)
   - If param_grid not provided, use `ParameterSweep.generate_default_grid(strategy_mode)`
   - Start sweep as background task
   - Return `JSONResponse({"task_id": task_id, "status": "running"})`

3. `POST /api/backtest/compare`:
   - Parse JSON body with: `symbol`, `start_date`, `end_date`
   - Create two configs (simple + composite) with same dates
   - Start comparison as background task using run_comparison()
   - Return `JSONResponse({"task_id": task_id, "status": "running"})`

4. `GET /api/backtest/status/{task_id}`:
   - Look up task_id in `request.app.state.backtest_tasks`
   - If task is done: return result as JSON (using BacktestResult.to_dict())
   - If task is running: return `{"status": "running"}`
   - If task not found: return 404
   - For sweep results: return SweepResult.to_dict()
   - For comparison: return dict with `simple` and `composite` result dicts

**Background task storage:** Initialize `app.state.backtest_tasks = {}` in `create_dashboard_app()`. Each task entry stores: `{"task": asyncio.Task, "type": "backtest"|"sweep"|"compare", "result": None|dict}`. When the task completes, store the result and clear the task reference.

Use a helper to wrap the async background execution:
```python
async def _run_backtest_task(task_id: str, app_state, config, db_path):
    try:
        result = await run_backtest(config, db_path)
        app_state.backtest_tasks[task_id]["result"] = result.to_dict()
        app_state.backtest_tasks[task_id]["status"] = "complete"
    except Exception as e:
        app_state.backtest_tasks[task_id]["result"] = {"error": str(e)}
        app_state.backtest_tasks[task_id]["status"] = "error"
```

**src/bot/dashboard/routes/pages.py**: Add backtest page route:
```python
@router.get("/backtest", response_class=HTMLResponse)
async def backtest_page(request: Request) -> HTMLResponse:
    templates: Jinja2Templates = request.app.state.templates
    # Get list of tracked pairs from data store for the symbol dropdown
    data_store = getattr(request.app.state, "data_store", None)
    tracked_pairs = []
    if data_store is not None:
        tracked_pairs = await data_store.get_tracked_pairs(active_only=True)
    return templates.TemplateResponse("backtest.html", {
        "request": request,
        "tracked_pairs": tracked_pairs,
    })
```

**src/bot/dashboard/app.py**:
1. Add `app.state.backtest_tasks = {}` in `create_dashboard_app()` after other state initialization.
2. Get the db_path from HistoricalDataSettings and store on app.state: `app.state.historical_db_path = "data/historical.db"` (this will be overridden by main.py wiring if needed).
  </action>
  <verify>
    Run `python -c "from bot.dashboard.app import create_dashboard_app; app = create_dashboard_app(); print('backtest_tasks' in dir(app.state))"` (should print True after app creation).
    Run `python -c "from bot.dashboard.routes.api import router; routes = [r.path for r in router.routes]; print([r for r in routes if 'backtest' in r])"` to verify backtest routes registered.
    Run `python -m pytest tests/ -x -q --timeout=30 2>&1 | tail -5` to verify no existing tests broken.
  </verify>
  <done>
    API endpoints POST /api/backtest/run, /api/backtest/sweep, /api/backtest/compare start background tasks and return task_id. GET /api/backtest/status/{task_id} returns running/complete/error status with results. /backtest page route renders backtest.html with tracked pairs for symbol selection.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create backtest dashboard templates with equity curve and heatmap</name>
  <files>
    src/bot/dashboard/templates/backtest.html
    src/bot/dashboard/templates/partials/backtest_form.html
    src/bot/dashboard/templates/partials/equity_curve.html
    src/bot/dashboard/templates/partials/param_heatmap.html
    src/bot/dashboard/templates/base.html
  </files>
  <action>
**src/bot/dashboard/templates/base.html**: Add "Backtest" link to the navigation bar.

In the `<nav>` element, after the title `<h1>`, add a navigation link:
```html
<div class="flex items-center gap-4">
    <a href="/" class="text-gray-400 hover:text-white text-sm">Dashboard</a>
    <a href="/backtest" class="text-gray-400 hover:text-white text-sm">Backtest</a>
</div>
```

Also add a `{% block head %}{% endblock %}` block in the `<head>` if not already present (it is present -- verify before editing).

**src/bot/dashboard/templates/backtest.html**: Main backtest page extending base.html.

```html
{% extends "base.html" %}

{% block title %}Backtest | Funding Rate Arbitrage{% endblock %}

{% block head %}
<script src="https://cdn.jsdelivr.net/npm/chart.js@4"></script>
{% endblock %}

{% block content %}
<div class="space-y-6">
    <h2 class="text-2xl font-bold text-white">Backtest Engine</h2>

    <!-- Configuration Form -->
    {% include "partials/backtest_form.html" %}

    <!-- Results Area (hidden until backtest completes) -->
    <div id="backtest-results" class="hidden space-y-6">
        <!-- Summary Metrics Cards -->
        <div id="metrics-cards" class="grid grid-cols-2 md:grid-cols-4 gap-4">
            <!-- Filled by JS after backtest completes -->
        </div>

        <!-- Equity Curve Chart -->
        {% include "partials/equity_curve.html" %}

        <!-- Comparison Table (shown for compare mode) -->
        <div id="comparison-section" class="hidden">
            <div class="bg-dash-card rounded-lg border border-dash-border p-4">
                <h3 class="text-white font-semibold mb-3">Strategy Comparison: Simple vs Composite</h3>
                <table id="comparison-table" class="w-full text-sm">
                    <thead>
                        <tr class="text-gray-400 border-b border-dash-border">
                            <th class="text-left py-2">Metric</th>
                            <th class="text-right py-2">Simple (v1.0)</th>
                            <th class="text-right py-2">Composite (v1.1)</th>
                        </tr>
                    </thead>
                    <tbody id="comparison-body"></tbody>
                </table>
            </div>
        </div>

        <!-- Parameter Heatmap (shown for sweep mode) -->
        {% include "partials/param_heatmap.html" %}
    </div>

    <!-- Loading Indicator -->
    <div id="backtest-loading" class="hidden text-center py-12">
        <div class="animate-spin inline-block w-8 h-8 border-2 border-gray-500 border-t-green-500 rounded-full"></div>
        <p class="text-gray-400 mt-3" id="loading-message">Running backtest...</p>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
// Backtest form submission and result polling logic
// ... (see JavaScript specification below)
</script>
{% endblock %}
```

The `{% block scripts %}` section should contain JavaScript that:
1. Handles form submission (intercepts submit, sends POST to /api/backtest/run or /sweep or /compare)
2. Polls /api/backtest/status/{task_id} every 2 seconds until complete
3. On completion: shows results area, populates metric cards, renders equity curve chart, renders comparison table or heatmap as appropriate
4. On error: shows error message

**src/bot/dashboard/templates/partials/backtest_form.html**: Configuration form.

Form with Tailwind-styled inputs:
- Symbol dropdown (populated from tracked_pairs): `<select name="symbol">{% for pair in tracked_pairs %}<option value="{{ pair.symbol }}">{{ pair.symbol }}</option>{% endfor %}</select>`. Include a text input fallback if no tracked pairs.
- Start date: `<input type="date" name="start_date">`
- End date: `<input type="date" name="end_date">`
- Strategy mode: `<select name="strategy_mode"><option value="simple">Simple (v1.0)</option><option value="composite">Composite (v1.1)</option></select>`
- Run mode: Radio buttons for "Single Backtest", "Compare v1.0 vs v1.1", "Parameter Sweep"
- Advanced parameters (collapsible section): min_funding_rate, entry_threshold, exit_threshold, weight overrides
- Submit button: "Run Backtest" with green accent color

Style: Match the existing dashboard dark theme (bg-dash-card, border-dash-border, text-gray-200, green accents for primary actions).

**src/bot/dashboard/templates/partials/equity_curve.html**: Chart.js equity curve.

```html
<div id="equity-curve-section" class="bg-dash-card rounded-lg border border-dash-border p-4">
    <h3 class="text-white font-semibold mb-2">Equity Curve</h3>
    <canvas id="equity-chart" height="300"></canvas>
</div>
```

JavaScript function `renderEquityCurve(equityData)` that:
- Creates a Chart.js line chart on the canvas
- X-axis: formatted dates from equity_curve timestamps
- Y-axis: equity value (P&L)
- Line color: green (#22c55e) for positive trend, red (#ef4444) gradient
- Dark theme: gray axis labels, transparent background
- Responsive: true
- For comparison mode: renders TWO lines (simple=blue, composite=green) on the same chart

**src/bot/dashboard/templates/partials/param_heatmap.html**: Parameter comparison heatmap.

Use an HTML table with inline background colors (NOT Chart.js matrix plugin -- simpler and more reliable):

```html
<div id="heatmap-section" class="hidden bg-dash-card rounded-lg border border-dash-border p-4">
    <h3 class="text-white font-semibold mb-2">Parameter Heatmap: Net P&L</h3>
    <div id="heatmap-container" class="overflow-x-auto">
        <!-- Populated by JavaScript -->
    </div>
</div>
```

JavaScript function `renderHeatmap(sweepData)` that:
- Takes the two-dimensional parameter grid and builds an HTML table
- Rows = first parameter values, Columns = second parameter values
- Cell values = net P&L for that combination
- Cell background: gradient from red (worst) to green (best) using inline rgba styles
- If >2 parameters: show a table with all combinations (one row per combo, columns for each param + metrics)

All templates use the existing Tailwind dark theme classes. No new CSS files.
  </action>
  <verify>
    Verify templates exist and have valid Jinja2 syntax:
    Run `python -c "from jinja2 import Environment, FileSystemLoader; env = Environment(loader=FileSystemLoader('src/bot/dashboard/templates')); t = env.get_template('backtest.html'); print('backtest.html OK')"`.
    Run `python -c "from jinja2 import Environment, FileSystemLoader; env = Environment(loader=FileSystemLoader('src/bot/dashboard/templates')); t = env.get_template('partials/backtest_form.html'); print('form OK')"`.
    Verify Chart.js CDN is included in backtest.html head block.
    Run `python -m pytest tests/ -x -q --timeout=30 2>&1 | tail -5` to verify no existing tests broken.
  </verify>
  <done>
    /backtest page renders with configuration form (symbol, dates, strategy, run mode). Equity curve displays Chart.js line chart with dark theme. Comparison mode shows two lines on same chart. Parameter heatmap renders as colored HTML table. Navigation bar includes Backtest link. All templates use existing Tailwind dark theme.
  </done>
</task>

</tasks>

<verification>
1. Navigate to /backtest page -- form renders with symbol dropdown, date pickers, strategy selector
2. Submit single backtest -- loading spinner appears, results show after completion
3. Equity curve chart renders with green line on dark background
4. Comparison mode shows two-line equity chart and comparison table
5. Sweep mode shows parameter heatmap with color-coded cells
6. Base template navigation includes Backtest link
7. All existing tests pass: `python -m pytest tests/ -x -q`
</verification>

<success_criteria>
- /backtest page loads without errors
- Form submits to API endpoint and polls for results
- Equity curve Chart.js chart renders correctly with historical timestamps
- Comparison table shows side-by-side metrics for simple vs composite
- Parameter heatmap shows color-coded net P&L for parameter combinations
- Navigation bar has Dashboard and Backtest links
- Background tasks don't block the main event loop
- Error states are handled gracefully (no data, failed backtest)
</success_criteria>

<output>
After completion, create `.planning/phases/06-backtest-engine/06-04-SUMMARY.md`
</output>
