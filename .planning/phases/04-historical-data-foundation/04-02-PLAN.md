---
phase: 04-historical-data-foundation
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/bot/data/store.py
  - src/bot/data/fetcher.py
autonomous: true

must_haves:
  truths:
    - "Historical funding rates can be inserted and queried from SQLite by symbol and time range"
    - "Historical OHLCV candles can be inserted and queried from SQLite by symbol and time range"
    - "Fetch state (earliest/latest timestamps) is tracked per symbol per data type for resume capability"
    - "Fetcher paginates backward through Bybit API using endTime, respecting 200-record limit for funding and 1000-record limit for OHLCV"
    - "Fetcher retries on API errors with exponential backoff (1s, 2s, 4s, 8s, 16s)"
    - "Duplicate records are silently ignored via INSERT OR IGNORE"
    - "Fetcher logs per-pair progress during bulk fetch"
  artifacts:
    - path: "src/bot/data/store.py"
      provides: "HistoricalDataStore with typed read/write methods"
      exports: ["HistoricalDataStore"]
      min_lines: 100
    - path: "src/bot/data/fetcher.py"
      provides: "HistoricalDataFetcher with pagination, retry, gap detection, progress logging"
      exports: ["HistoricalDataFetcher"]
      min_lines: 150
  key_links:
    - from: "src/bot/data/store.py"
      to: "src/bot/data/database.py"
      via: "HistoricalDatabase.db property for SQL execution"
      pattern: "self\\._database\\.db"
    - from: "src/bot/data/store.py"
      to: "src/bot/data/models.py"
      via: "Returns HistoricalFundingRate and OHLCVCandle instances"
      pattern: "HistoricalFundingRate|OHLCVCandle"
    - from: "src/bot/data/fetcher.py"
      to: "src/bot/data/store.py"
      via: "Writes fetched data to store"
      pattern: "self\\._store\\."
    - from: "src/bot/data/fetcher.py"
      to: "src/bot/exchange/client.py"
      via: "Calls fetch_funding_rate_history and fetch_ohlcv"
      pattern: "self\\._exchange\\.fetch_funding_rate_history|self\\._exchange\\.fetch_ohlcv"
    - from: "src/bot/data/fetcher.py"
      to: "asyncio.sleep"
      via: "Exponential backoff retry and batch delay"
      pattern: "asyncio\\.sleep"
---

<objective>
Build the HistoricalDataStore (typed SQLite read/write layer) and HistoricalDataFetcher (paginated API fetch pipeline with retry, resume, and progress logging).

Purpose: These are the core data pipeline components. The store abstracts all SQL behind typed methods. The fetcher orchestrates the paginated backward-walk through Bybit's API, handles retries, tracks fetch state for resumption, and logs per-pair progress. Together they implement DATA-01 through DATA-04.

Output: Two production-ready modules that can fetch 1 year of historical data for 20 pairs, store it in SQLite, and resume from where they left off on restart.
</objective>

<execution_context>
@/Users/luckleineschaars/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luckleineschaars/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-historical-data-foundation/04-CONTEXT.md
@.planning/phases/04-historical-data-foundation/04-RESEARCH.md
@.planning/phases/04-historical-data-foundation/04-01-SUMMARY.md
@src/bot/data/models.py
@src/bot/data/database.py
@src/bot/exchange/client.py
@src/bot/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: HistoricalDataStore -- typed SQLite read/write abstraction</name>
  <files>src/bot/data/store.py</files>
  <action>
Create `src/bot/data/store.py` with class `HistoricalDataStore`:

Constructor: Takes `database: HistoricalDatabase` (from plan 01). Stores as `self._database`.

**Write methods:**

1. `async insert_funding_rates(records: list[dict]) -> int`:
   - Accepts ccxt-format dicts with keys: symbol, fundingRate, timestamp, info
   - Extract interval_hours from `record["info"].get("fundingIntervalHours", 8)` if available, otherwise default to 8
   - Convert to tuples: `(symbol, timestamp, str(fundingRate), interval_hours)`
   - Use `executemany` with `INSERT OR IGNORE INTO funding_rate_history (symbol, timestamp_ms, funding_rate, interval_hours) VALUES (?, ?, ?, ?)`
   - Commit and return `cursor.rowcount` (actually inserted, excludes ignored duplicates)

2. `async insert_ohlcv_candles(symbol: str, candles: list[list]) -> int`:
   - Accepts ccxt-format lists: [timestamp_ms, open, high, low, close, volume]
   - Convert to tuples: `(symbol, candle[0], str(candle[1]), str(candle[2]), str(candle[3]), str(candle[4]), str(candle[5]))`
   - Use `executemany` with `INSERT OR IGNORE INTO ohlcv_candles (symbol, timestamp_ms, open, high, low, close, volume) VALUES (?, ?, ?, ?, ?, ?, ?)`
   - Commit and return rowcount

3. `async update_fetch_state(symbol: str, data_type: str, earliest_ms: int, latest_ms: int) -> None`:
   - Use `INSERT OR REPLACE INTO fetch_state (symbol, data_type, earliest_ms, latest_ms, last_fetched_at) VALUES (?, ?, ?, ?, ?)` with current time in ms
   - Commit

4. `async update_tracked_pair(symbol: str, volume_24h: Decimal, is_active: bool = True) -> None`:
   - Use `INSERT OR REPLACE INTO tracked_pairs (symbol, added_at, last_volume_24h, is_active) VALUES (?, COALESCE((SELECT added_at FROM tracked_pairs WHERE symbol = ?), ?), ?, ?)` preserving original added_at
   - Commit

**Read methods:**

5. `async get_fetch_state(symbol: str, data_type: str) -> dict | None`:
   - Returns dict with earliest_ms, latest_ms, last_fetched_at or None if no state exists
   - Query: `SELECT earliest_ms, latest_ms, last_fetched_at FROM fetch_state WHERE symbol = ? AND data_type = ?`

6. `async get_funding_rates(symbol: str, since_ms: int | None = None, until_ms: int | None = None) -> list[HistoricalFundingRate]`:
   - Build WHERE clause dynamically based on provided parameters
   - Order by timestamp_ms ASC
   - Return list of HistoricalFundingRate dataclass instances (convert TEXT back to Decimal)

7. `async get_ohlcv_candles(symbol: str, since_ms: int | None = None, until_ms: int | None = None) -> list[OHLCVCandle]`:
   - Same pattern as get_funding_rates but for candles table
   - Return list of OHLCVCandle instances

8. `async get_tracked_pairs(active_only: bool = True) -> list[dict]`:
   - Returns list of dicts with symbol, added_at, last_volume_24h, is_active
   - If active_only, filter WHERE is_active = 1

9. `async get_data_status() -> dict`:
   - Returns aggregate status dict for dashboard: total_pairs (count of active tracked_pairs), total_funding_records (count from funding_rate_history), total_ohlcv_records (count from ohlcv_candles), earliest_date_ms (min timestamp across both tables), latest_date_ms (max timestamp across both tables), last_sync_ms (max last_fetched_at from fetch_state)
   - Use efficient COUNT(*) and MIN/MAX queries

Use structlog `get_logger(__name__)` for logging. All SQL access through `self._database.db` (the aiosqlite Connection). Always `await self._database.db.commit()` after writes.
  </action>
  <verify>
    - `python -c "from bot.data.store import HistoricalDataStore; print('store OK')"` succeeds
    - Write a quick inline test: create HistoricalDatabase + HistoricalDataStore with temp db, connect, insert some mock funding rates, query them back, verify count matches. Run via `python -c "..."` multiline script.
    - Existing tests pass: `python -m pytest tests/ -x -q`
  </verify>
  <done>
    - HistoricalDataStore has all 9 methods (4 write, 5 read)
    - INSERT OR IGNORE deduplicates records on composite primary key
    - Fetch state tracks earliest/latest timestamps per symbol per data type
    - All Decimal values stored as TEXT and restored as Decimal on read
    - Data status method returns aggregate counts for dashboard
  </done>
</task>

<task type="auto">
  <name>Task 2: HistoricalDataFetcher -- paginated fetch pipeline with retry and resume</name>
  <files>src/bot/data/fetcher.py</files>
  <action>
Create `src/bot/data/fetcher.py` with class `HistoricalDataFetcher`:

Constructor: Takes `exchange: ExchangeClient`, `store: HistoricalDataStore`, `settings: HistoricalDataSettings`. Store all three.

**Core public methods:**

1. `async ensure_data_ready(symbols: list[str], progress_callback: Callable | None = None) -> None`:
   - The main entry point called on startup. Blocks until all symbols have complete historical data.
   - For each symbol (enumerate with 1-based index):
     - Log: `"fetching_historical_data", symbol=symbol, progress=f"{i}/{len(symbols)}"`
     - Call `_fetch_funding_history(symbol)`
     - Call `_fetch_ohlcv_history(symbol)`
     - Update tracked_pairs via store
     - If `progress_callback` is not None, call `await progress_callback(symbol, i, len(symbols))` for dashboard live progress
   - Log summary when done: `"historical_data_ready", pairs=len(symbols), total_duration_seconds=...`

2. `async incremental_update(symbols: list[str]) -> None`:
   - Called on each scan cycle. Fetches only new records since last fetch.
   - For each symbol, fetch from latest_ms to now (no backward pagination needed -- just latest page).
   - Log at DEBUG level per pair, INFO level for summary (per user decision on Claude's discretion: signal-to-noise balance).

**Internal fetch methods:**

3. `async _fetch_funding_history(symbol: str) -> None`:
   - Check fetch_state for this symbol + "funding". If state exists, resume from there.
   - Calculate `since_ms` from settings.lookback_days (1 year default): `int(time.time() * 1000) - settings.lookback_days * 86400 * 1000`
   - Calculate `until_ms` = now in milliseconds
   - If state.earliest_ms exists and <= since_ms: data is complete, only fetch new data from state.latest_ms to now
   - If state.earliest_ms exists and > since_ms: need to fetch backward from state.earliest_ms to since_ms, AND forward from state.latest_ms to now
   - If no state: full backward fetch from until_ms to since_ms
   - Use `_fetch_funding_rates_paginated()` for backward walks
   - After fetching, update fetch_state with new earliest/latest

4. `async _fetch_ohlcv_history(symbol: str) -> None`:
   - Same pattern as _fetch_funding_history but for OHLCV candles
   - Use settings.ohlcv_interval ("1h")
   - Use `_fetch_ohlcv_paginated()` for backward walks

5. `async _fetch_funding_rates_paginated(symbol: str, since_ms: int, until_ms: int) -> int`:
   - Walk BACKWARD from until_ms to since_ms using endTime parameter
   - Each call: `await self._fetch_with_retry(self._exchange.fetch_funding_rate_history, symbol, limit=200, params={"endTime": current_end})`
   - After each batch:
     - Filter to only records where timestamp >= since_ms
     - Insert into store via `store.insert_funding_rates(batch)`
     - Find oldest timestamp in batch, use as next current_end
     - Sleep settings.fetch_batch_delay (0.1s safety margin between calls)
     - If batch is empty or oldest_ts >= current_end: break (no progress guard)
   - Log progress: `"funding_fetch_progress", symbol=symbol, records_fetched=total, oldest_date=...`
   - Return total records inserted

6. `async _fetch_ohlcv_paginated(symbol: str, since_ms: int, until_ms: int) -> int`:
   - Same backward pagination pattern but with limit=1000
   - IMPORTANT: Bybit kline response is REVERSE-SORTED (newest first). After each batch, reverse it before finding the oldest timestamp. See research pitfall #3.
   - Use `params={"endTime": current_end}` with `self._exchange.fetch_ohlcv(symbol, timeframe=self._settings.ohlcv_interval, limit=1000, params=...)`
   - Insert via `store.insert_ohlcv_candles(symbol, batch)`
   - Return total records inserted

7. `async _fetch_with_retry(fetch_fn, *args, **kwargs) -> list`:
   - Exponential backoff retry wrapper
   - max_retries from settings (default 5)
   - Base delay from settings (default 1.0s), doubles each attempt: 1, 2, 4, 8, 16
   - Log each retry: `"fetch_retry", attempt=N, max_retries=M, delay=D, error=str(e)`
   - Re-raise on final failure
   - Catch broad Exception but specifically handle ccxt rate limit errors gracefully (longer delay)

CRITICAL implementation notes:
- Always pass `endTime` to Bybit funding rate history -- passing only `startTime`/`since` causes errors (research pitfall #1)
- Dynamic funding intervals: do NOT assume 8h. The interval is stored per record from the API response (research pitfall #2)
- ccxt symbol format: use unified symbols like `BTC/USDT:USDT` everywhere, ccxt converts internally (research pitfall #5)
- Single aiosqlite Connection: the HistoricalDatabase owns the connection, store uses it -- no concurrent write issues (research pitfall #6)
  </action>
  <verify>
    - `python -c "from bot.data.fetcher import HistoricalDataFetcher; print('fetcher OK')"` succeeds
    - Unit-level verification: Create a mock exchange client, mock store, create fetcher, verify _fetch_with_retry retries on exception and succeeds after transient failure (write inline test script)
    - Verify pagination logic: Create test with mock exchange returning 2 batches of funding rates, verify fetcher calls exchange twice with correct endTime progression
    - Existing tests pass: `python -m pytest tests/ -x -q`
  </verify>
  <done>
    - HistoricalDataFetcher.ensure_data_ready() fetches full history for a list of symbols with per-pair progress logging
    - Backward pagination walks from now to lookback_days ago using endTime parameter
    - Retry with exponential backoff (5 retries, 1s base delay)
    - Resume from last fetch_state on restart (no redundant re-fetching)
    - Incremental updates fetch only new records since last sync
    - 100ms batch delay between paginated API calls for rate limit safety
    - Duplicate records silently ignored via store's INSERT OR IGNORE
  </done>
</task>

</tasks>

<verification>
- Full data pipeline test: Create HistoricalDatabase + Store + mock Exchange + Fetcher with temp DB, call ensure_data_ready with mock data, verify records in database, verify fetch_state updated
- Resume test: After initial fetch, call ensure_data_ready again, verify no redundant API calls (fetch_state check prevents re-fetch)
- All existing tests pass: `python -m pytest tests/ -x -q`
</verification>

<success_criteria>
- HistoricalDataStore provides typed CRUD for funding rates, OHLCV candles, fetch state, tracked pairs
- HistoricalDataFetcher paginates backward through Bybit API with proper endTime handling
- Retry with exponential backoff on API errors (1s, 2s, 4s, 8s, 16s delays)
- Resume capability: fetcher checks fetch_state and only fetches missing data
- Deduplication via INSERT OR IGNORE on composite primary keys
- Per-pair progress logging during bulk fetch
- Data status query available for dashboard integration
</success_criteria>

<output>
After completion, create `.planning/phases/04-historical-data-foundation/04-02-SUMMARY.md`
</output>
