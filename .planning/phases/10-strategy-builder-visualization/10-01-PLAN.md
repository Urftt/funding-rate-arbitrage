---
phase: 10-strategy-builder-visualization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/bot/backtest/runner.py
  - src/bot/backtest/models.py
  - src/bot/dashboard/routes/api.py
  - src/bot/dashboard/templates/partials/backtest_form.html
  - src/bot/dashboard/templates/backtest.html
autonomous: true

must_haves:
  truths:
    - "User can select 'Multi-Pair' run mode and see checkboxes for all tracked pairs"
    - "User can run the same backtest config across multiple selected pairs simultaneously"
    - "User can see a comparison table with pair, net P&L, Sharpe, win rate for all pairs"
    - "User can see 'X of Y pairs profitable' aggregate summary above the comparison table"
    - "Pairs that fail (no data) show 'No data' in the table instead of crashing the entire run"
  artifacts:
    - path: "src/bot/backtest/runner.py"
      provides: "run_multi_pair() async function"
      contains: "async def run_multi_pair"
    - path: "src/bot/backtest/models.py"
      provides: "MultiPairResult dataclass"
      contains: "class MultiPairResult"
    - path: "src/bot/dashboard/routes/api.py"
      provides: "/api/backtest/multi endpoint and _run_multi_pair_task"
      contains: "_run_multi_pair_task"
    - path: "src/bot/dashboard/templates/partials/backtest_form.html"
      provides: "Multi-Pair radio option and pair selection checkboxes"
      contains: "multi"
    - path: "src/bot/dashboard/templates/backtest.html"
      provides: "displayMultiPairResult function and multi-pair comparison table"
      contains: "displayMultiPairResult"
  key_links:
    - from: "src/bot/dashboard/templates/backtest.html"
      to: "/api/backtest/multi"
      via: "fetch POST in btn click handler"
      pattern: "api/backtest/multi"
    - from: "src/bot/dashboard/routes/api.py"
      to: "src/bot/backtest/runner.py"
      via: "_run_multi_pair_task calls run_multi_pair"
      pattern: "run_multi_pair"
    - from: "src/bot/backtest/runner.py"
      to: "src/bot/backtest/runner.py"
      via: "run_multi_pair calls run_backtest in a loop"
      pattern: "await run_backtest"
---

<objective>
Add multi-pair backtest execution: the user selects multiple pairs, runs the same backtest config across all of them, and sees a unified comparison table with aggregate summary.

Purpose: Satisfies STRT-01 (multi-pair simultaneous backtest), STRT-02 (unified comparison table), and STRT-04 (aggregate "X of Y profitable" summary).
Output: Multi-pair backtest backend + API + frontend UI with comparison table.
</objective>

<execution_context>
@/Users/luckleineschaars/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luckleineschaars/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/bot/backtest/runner.py
@src/bot/backtest/models.py
@src/bot/backtest/sweep.py
@src/bot/dashboard/routes/api.py
@src/bot/dashboard/routes/pages.py
@src/bot/dashboard/templates/backtest.html
@src/bot/dashboard/templates/partials/backtest_form.html
</context>

<tasks>

<task type="auto">
  <name>Task 1: MultiPairResult model and run_multi_pair() function</name>
  <files>src/bot/backtest/models.py, src/bot/backtest/runner.py</files>
  <action>
  In `src/bot/backtest/models.py`, add a `MultiPairResult` dataclass after the `SweepResult` class:

  ```python
  @dataclass
  class MultiPairResult:
      """Results from running the same config across multiple pairs."""
      symbols: list[str]
      base_config: BacktestConfig
      results: list[tuple[str, BacktestResult | None, str | None]]
      # Each tuple: (symbol, result_or_None, error_or_None)

      @property
      def profitable_count(self) -> int:
          return sum(1 for _, r, e in self.results if r and r.metrics.net_pnl > Decimal("0"))

      @property
      def total_count(self) -> int:
          return len(self.results)

      @property
      def successful_count(self) -> int:
          return sum(1 for _, r, _ in self.results if r is not None)

      def to_dict(self) -> dict:
          items = []
          for symbol, result, error in self.results:
              if error:
                  items.append({"symbol": symbol, "error": error, "metrics": None})
              elif result:
                  items.append({"symbol": symbol, "error": None, "metrics": result.to_dict()["metrics"]})
              else:
                  items.append({"symbol": symbol, "error": "Unknown error", "metrics": None})
          return {
              "symbols": self.symbols,
              "config": self.base_config.to_dict(),
              "results": items,
              "profitable_count": self.profitable_count,
              "total_count": self.total_count,
              "successful_count": self.successful_count,
          }
  ```

  In `src/bot/backtest/runner.py`, add `run_multi_pair()` after `run_comparison()`. Import `MultiPairResult` from models. Pattern follows `ParameterSweep.run()` -- sequential loop with per-pair error handling:

  ```python
  async def run_multi_pair(
      symbols: list[str],
      base_config: BacktestConfig,
      db_path: str = "data/historical.db",
      fee_settings: FeeSettings | None = None,
      backtest_settings: BacktestSettings | None = None,
  ) -> MultiPairResult:
      """Run the same backtest config across multiple pairs sequentially."""
      if fee_settings is None:
          fee_settings = FeeSettings()
      if backtest_settings is None:
          backtest_settings = BacktestSettings()

      logger.info("run_multi_pair_starting", symbols=symbols, total=len(symbols))
      start_time = time.monotonic()
      results = []

      for symbol in symbols:
          try:
              config = base_config.with_overrides(symbol=symbol)
              result = await run_backtest(config, db_path, fee_settings, backtest_settings)
              # Memory management: only keep metrics, discard equity curve and trades
              compact = BacktestResult(
                  config=result.config,
                  equity_curve=[],
                  trades=[],
                  trade_stats=result.trade_stats,
                  metrics=result.metrics,
              )
              results.append((symbol, compact, None))
          except Exception as e:
              logger.warning("multi_pair_single_error", symbol=symbol, error=str(e))
              results.append((symbol, None, str(e)))

      elapsed = time.monotonic() - start_time
      logger.info("run_multi_pair_complete", total=len(symbols), elapsed_seconds=round(elapsed, 2))
      return MultiPairResult(symbols=symbols, base_config=base_config, results=results)
  ```

  Import `MultiPairResult` at the top of runner.py alongside existing model imports.
  </action>
  <verify>
  Run `python -c "from bot.backtest.models import MultiPairResult; print('OK')"` and `python -c "from bot.backtest.runner import run_multi_pair; print('OK')"` from the project root. Both should print OK.
  </verify>
  <done>MultiPairResult dataclass exists with to_dict(), profitable_count, total_count properties. run_multi_pair() function exists, loops over symbols calling run_backtest(), handles per-pair errors gracefully.</done>
</task>

<task type="auto">
  <name>Task 2: Multi-pair API endpoint, form UI, and comparison table display</name>
  <files>src/bot/dashboard/routes/api.py, src/bot/dashboard/templates/partials/backtest_form.html, src/bot/dashboard/templates/backtest.html</files>
  <action>
  **API endpoint** -- In `api.py`, add `_run_multi_pair_task()` background task and `POST /backtest/multi` endpoint. Follow the exact same pattern as `_run_sweep_task()`:

  1. Import `run_multi_pair` from `bot.backtest.runner`.
  2. Add `_run_multi_pair_task(task_id, app_state, symbols, config, db_path)` that calls `run_multi_pair()` and stores `result.to_dict()` in `app_state.backtest_tasks[task_id]`.
  3. Add `@router.post("/backtest/multi")` endpoint that:
     - Reads `symbols` (list of strings) from request body
     - Validates `symbols` is non-empty
     - Reads `start_date`, `end_date` from body, parses with `_parse_dates()`
     - Builds base config via `_build_config_from_body()` (using first symbol as placeholder)
     - Creates background task with `asyncio.create_task()`
     - Stores task with `type: "multi"`
     - Returns `{"task_id": ..., "status": "running"}`

  **Form UI** -- In `backtest_form.html`:
  1. Add a 4th radio option in the Run Mode group:
     ```html
     <label class="flex items-center gap-2 text-sm text-gray-200 cursor-pointer">
         <input type="radio" name="run_mode" value="multi"
                class="text-green-500 focus:ring-green-500 bg-dash-bg border-dash-border">
         Multi-Pair
     </label>
     ```
  2. After the Run Mode div, add a new conditional panel for pair selection (hidden by default, shown when "multi" selected):
     ```html
     <div id="multi-pair-panel" class="mt-3 hidden">
       <label class="block text-xs text-gray-400 mb-2">Select Pairs</label>
       <div class="flex items-center gap-3 mb-2">
         <button type="button" id="btn-select-all" class="text-xs text-blue-400 hover:text-blue-300">Select All</button>
         <button type="button" id="btn-deselect-all" class="text-xs text-blue-400 hover:text-blue-300">Deselect All</button>
       </div>
       <div class="grid grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-2" id="multi-pair-checkboxes">
         {% if tracked_pairs %}
         {% for pair in tracked_pairs %}
         <label class="flex items-center gap-2 text-sm text-gray-300 cursor-pointer">
           <input type="checkbox" name="multi_symbols" value="{{ pair.symbol }}"
                  class="text-green-500 focus:ring-green-500 bg-dash-bg border-dash-border rounded">
           {{ pair.symbol.split('/')[0] }}
         </label>
         {% endfor %}
         {% endif %}
       </div>
     </div>
     ```

  **Backtest.html JS** -- Update the script section:
  1. Add DOM references: `const multiPairPanel = document.getElementById('multi-pair-panel');`
  2. Add run mode change listener to show/hide multi-pair panel:
     ```javascript
     form.querySelectorAll('input[name="run_mode"]').forEach(function(radio) {
         radio.addEventListener('change', function() {
             multiPairPanel.classList.toggle('hidden', this.value !== 'multi');
         });
     });
     ```
  3. Wire select-all/deselect-all buttons.
  4. Update `getEndpoint()` to handle `'multi'` returning `'/api/backtest/multi'`.
  5. Update `getFormData()`: when run_mode is 'multi', collect checked symbols from checkboxes into `data.symbols` array instead of using the single symbol dropdown.
  6. Update `validateForm()`: when run_mode is 'multi', validate at least one pair is checked.
  7. Update `pollStatus()` result handler: add `else if (taskType === 'multi')` calling `displayMultiPairResult(data.result)`.
  8. Update loading message for multi mode: `'Running multi-pair backtest across ' + data.symbols.length + ' pairs...'`.
  9. Add `displayMultiPairResult(result)` function:
     - Shows aggregate summary card at top: "X of Y pairs profitable" as a metric card.
     - Builds a comparison table with columns: Pair, Net P&L, Sharpe Ratio, Win Rate, Total Trades, Status.
     - Rows sorted by net P&L descending.
     - Error rows show "No data" in a gray/italic style.
     - Hides trade log, trade stats, P&L histogram, comparison section, heatmap.
     - Shows equity section as hidden (no per-pair equity curve in multi mode).
  10. In the `btn.addEventListener('click')` handler, update the taskType derivation to include 'multi'.
  11. In `resetResults()`, also handle hiding any multi-pair-specific sections.

  Add a new HTML section for multi-pair results in backtest.html inside `#backtest-results`:
  ```html
  <!-- Multi-Pair Comparison Table (shown for multi mode) -->
  <div id="multi-pair-section" class="hidden">
      <div class="bg-dash-card rounded-lg border border-dash-border p-4">
          <h3 class="text-white font-semibold mb-1">Multi-Pair Results</h3>
          <p class="text-sm text-gray-400 mb-3" id="multi-pair-summary"></p>
          <table class="w-full text-sm">
              <thead>
                  <tr class="text-gray-400 border-b border-dash-border">
                      <th class="text-left py-2 px-2">Pair</th>
                      <th class="text-right py-2 px-2">Net P&L</th>
                      <th class="text-right py-2 px-2">Sharpe</th>
                      <th class="text-right py-2 px-2">Win Rate</th>
                      <th class="text-right py-2 px-2">Trades</th>
                      <th class="text-right py-2 px-2">Funding</th>
                      <th class="text-right py-2 px-2">Fees</th>
                  </tr>
              </thead>
              <tbody id="multi-pair-body"></tbody>
          </table>
      </div>
  </div>
  ```

  Include `multiPairSection` in `resetResults()` to hide and clear it.
  </action>
  <verify>
  1. Start the dashboard: `python -m bot.main` (or check syntax with `python -c "from bot.dashboard.routes.api import router; print('OK')"`).
  2. Visit `/backtest` and verify the "Multi-Pair" radio option appears.
  3. Select "Multi-Pair" and verify checkboxes appear for all tracked pairs.
  4. Confirm Select All / Deselect All buttons work.
  </verify>
  <done>
  POST /api/backtest/multi endpoint accepts symbols list and runs multi-pair backtest as background task. Backtest form has "Multi-Pair" radio option with checkbox pair selection panel. Multi-pair results display in a comparison table showing pair, net P&L, Sharpe, win rate with "X of Y pairs profitable" aggregate summary.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from bot.backtest.models import MultiPairResult; from bot.backtest.runner import run_multi_pair; print('imports OK')"` succeeds
2. The backtest page shows 4 run mode options: Single, Compare, Sweep, Multi-Pair
3. Selecting Multi-Pair shows pair checkboxes with Select All / Deselect All
4. Running a multi-pair backtest returns results polled via /api/backtest/status/{task_id}
5. Results display a comparison table with aggregate "X of Y profitable" summary
6. Pairs with no data show graceful "No data" instead of crashing
</verification>

<success_criteria>
- MultiPairResult dataclass with to_dict(), profitable_count, total_count
- run_multi_pair() in runner.py with per-pair error handling and memory management
- POST /api/backtest/multi endpoint with background task pattern
- Multi-Pair radio option in backtest form with checkbox pair selection
- Comparison table showing all pairs with metrics sorted by P&L
- Aggregate summary "X of Y pairs profitable"
</success_criteria>

<output>
After completion, create `.planning/phases/10-strategy-builder-visualization/10-01-SUMMARY.md`
</output>
